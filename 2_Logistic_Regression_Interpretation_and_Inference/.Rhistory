#| code-fold: show
# package used to import spss file
library(foreign)
don_desbois <- read.spss("C:/Users/Alex/Documents/GitHub/Scoring_Alex/teaching_Scoring/data/Agriculture Farm Lending/desbois.sav",
to.data.frame = TRUE) %>% as_tibble()
don_desbois <- don_desbois %>%
mutate(Y = as.factor(if_else(DIFF=='healthy', 0, 1))) %>%
dplyr::select(-DIFF)
glimpse(don_desbois)
# Variable definitions from Desbois article
# Capitalization
# r1 total debt / total assets;
# r2 stockholders' equity / invested capital;
# r3 short term debt / total debt;
# r4 short term debt / total assets;
# r5 long and medium term debt / total assets;
# Weight of the debt
# r6 total debt / gross product;
# r7 long and medium term debt / gross product;
# r8 short term debt / gross product;
# Liquidity
# r11 working capital / gross product;
# r12 working capital / (real inputs - financial expenses);
# r14 short term debt / circulating assets;
# Debt servicing
# r17 financial expenses / total debt;
# r18 financial expenses / gross product;
# r19 (financial expenses+ refunding of long and
#      medium term capital) / gross product;
# r21 financial expenses / EBITDA;
# r22 (financial expenses + refunding of long and
#      medium term capital)/EBITDA;
# Capital profitability
# r24 EBITDA /total assets;
# Earnings
# r28 EBITDA / gross product;
# r30 available income / gross product;
# r32 (EBITDA - financial expenses) / gross product;
# Productive activity
# r36 immobilized assets / gross product;
# r37 gross product / total assets.
view(don_desbois)
#| code-fold: true
vars_quanti <- names(don_desbois %>% select_if(is.numeric))
for(var in vars_quanti){
var <- as.name(var)
print(ggplot(don_desbois %>%
# filter(r17 <0.14) %>%
mutate(Y = as.numeric(Y)-1), aes(x=!! var,y = Y)) +
geom_jitter(height = 0.1, width = 0) +
geom_smooth(method = "glm",
formula = y ~ x,
method.args = list(family = "binomial"),
se = FALSE,
col = "dodgerblue"))
}
?filter
?Winsorize
??Winsorize
?across
View(vars_quali)
#| code-fold: true
vars_quali <- names(don_desbois %>% select_if(is.factor) %>% select(-Y))
for(var in vars_quali){
var <- as.name(var)
print(ggplot(don_desbois %>%
group_by(!!var, Y) %>%
summarize(count = n()) %>%
ungroup()) +
geom_bar(aes(x = Y, y = count, fill = !!var), position="dodge",stat="identity"))
}
names(don_desbois %>% select_if(is.factor) %>% select(-Y))
#| code-fold: true
res.pca = FactoMineR::PCA(don_desbois,
scale.unit = TRUE,
quanti.sup = c(3, 6),
quali.sup = c(1, 2, 4, 5, 7, 30),
ncp = 5, graph=TRUE)
FactoMineR::dimdesc(res.pca, axes=c(1,2))
??PCA
?FactoMineR::dimdesc
#| code-fold: true
# Similar to Desbois Fig 2.
# Plot of the farm holdings in the first factorial plane of the normalized PCA based on financial ratios
# with illustrative variable Y (0=”healthy”; 1=”failing”)
FactoMineR::plot.PCA(res.pca, axes=c(1, 2), choix="ind", habillage=30, invisible = c("quali"))
FactoMineR::plot.PCA(res.pca, axes=c(1, 2), choix="ind", habillage=30, invisible = c("ind"))
?FactoMineR::plot.PCA
#| code-fold: show
# Looking at summary outputs
(sum_desbois <- summary(glm_desbois_full))
#| code-fold: true
glm_desbois_full <- glm(Y~., data = don_desbois, family=binomial())
summary(glm_desbois_full)
#| code-fold: show
# Looking at summary outputs
(sum_desbois <- summary(glm_desbois_full))
#| code-fold: show
# Testing the r36 coefficient (Terms = 40)
aod::wald.test(b = coef(glm_desbois_full), Sigma = vcov(glm_desbois_full), Terms = 40)
?aod::wald.test
#| code-fold: show
# manually computing from beta/hessian
beta_r17 <- sum_desbois$coefficients[40,1]
stdev_r17 <- sum_desbois$coefficients[40,2]
wald <- beta_r17 ^ 2 / stdev_r17 ^ 2
1-pchisq(wald, df = 1)
z_val <- sum_desbois$coefficients[40,3]
z_val
2*(1-pnorm(abs(z_val)))
sum_desbois$coefficients
#| code-fold: show
# manually computing from beta/hessian
beta_r37 <- sum_desbois$coefficients[40,1]
stdev_r37 <- sum_desbois$coefficients[40,2]
wald <- beta_r37 ^ 2 / stdev_r37 ^ 2
1-pchisq(wald, df = 1)
z_val <- sum_desbois$coefficients[40,3]
z_val
2*(1-pnorm(abs(z_val)))
#| code-fold: show
# using anova on two models (with/without  r36)
glm_desbois_wo_r36 <- glm(Y~., data = don_desbois %>% select(-r36), family=binomial())
anova(glm_desbois_wo_r36, glm_desbois_full, test= "LRT")
#| code-fold: show
glmtoolbox::hltest(glm_desbois_full)
#| code-fold: show
#
# Due to outliers some predictions "saturate"  to 0 or 1
# Warning: glm.fit: fitted probabilities numerically 0 or 1 occurredWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred
# winsorizing financial ratios removes the issue
don_desbois_winsorized <- don_desbois %>%
mutate(across(r1:r37, ~ DescTools::Winsorize(.x , quantile(.x, probs = c(0.025, 0.975)))))
# data_afl <- don_desbois_winsorized
data_afl <- don_desbois
#define intercept-only model
intercept_only <- glm(Y ~ 1, data=data_afl, family="binomial")
#define model with all predictors
# In Desbois only financial ratios are used
all <- glm(Y ~ ., data=data_afl %>% select(Y, starts_with('r')), family="binomial")
# We use all variables
# all <- glm(Y ~ ., data=data_afl , family="binomial")
#| code-fold: show
# perform forward stepwise regression
forward_aic <- step(intercept_only, direction='forward', test = 'LRT', scope=formula(all), k=2, trace = FALSE)
summary(forward_aic)
#| code-fold: show
all <- glm(Y ~ ., data=data_afl %>% select(Y, starts_with('r')), family="binomial")
first_step <- add1(intercept_only, scope = formula(all), test = "LRT")
first_step <- first_step %>%
tibble() %>%
add_column(variable=row.names(first_step)) %>%
arrange(desc(LRT))
first_step
#| code-fold: show
second_step <- add1(update(intercept_only, ~. + r1), scope = formula(all), test = "LRT")
second_step <- second_step %>%
tibble() %>%
add_column(variable=row.names(second_step)) %>%
arrange(desc(LRT))
second_step
#| code-fold: show
# perform forward stepwise regression
forward_aic <- step(intercept_only, direction='forward', test = 'LRT', scope=formula(all), k=2, trace = FALSE)
summary(forward_aic)
#| code-fold: show
forward_bic <- step(intercept_only, direction='forward', test = 'LRT', scope=formula(all), k=log(nrow(don_desbois)), trace = FALSE)
summary(forward_bic)
#| code-fold: show
backward_bic <- step(all, direction='backward', test = 'LRT', scope=formula(all), k=log(nrow(don_desbois)), trace = FALSE)
summary(backward_bic)
#| code-fold: show
#define intercept-only model
intercept_only_w <- glm(Y ~ 1, data=don_desbois_winsorized, family="binomial")
#define model with all predictors
all_w <- glm(Y ~ ., data=don_desbois_winsorized, family="binomial")
#perform forward stepwise regression AIC
forward_aic_w <- step(intercept_only_w, direction='forward', scope=formula(all_w), k=2, trace = FALSE)
#perform forward stepwise regression BIC
forward_bic_w <- step(intercept_only_w, direction='forward', scope=formula(all_w), k=log(nrow(don_desbois_winsorized)), trace = FALSE)
#| code-fold: show
summary(forward_aic_w)
#| code-fold: show
Y <- don_desbois %>% pull(Y)
desbois_lasso <- glmnetUtils::glmnet(Y ~ ., data=don_desbois, family="binomial", alpha=1)
plot(desbois_lasso)
#| code-fold: show
lasso_result <- as_tibble(as.matrix(cbind(desbois_lasso$lambda, t(desbois_lasso$beta))))
names(lasso_result) <-  c("lambda", row.names(desbois_lasso$beta))
lasso_result
?purrr::map
?unnest
?pROC::roc
library(tidyverse)
library(tidyverse)
# Class 1 parameters
pi_1 <- 0.5
mu_11 <- 4
mu_12 <- 0
sig_11 <- sqrt(2)
sig_12 <- 2
rho_1 <- 0
# Class 0 parameters , choosen randomly with prob 1/2 of 2 buckets
pi_0 <- 1- pi_1
mu_01a <- 1
mu_02a <- 4
mu_01b <- 1
mu_02b <- -4
sig_01 <- sqrt(2)
sig_02 <- sqrt(2)
rho_0 <- 0
# Class 1 helpers
mu_1 <- c(mu_11, mu_12)
mu_1
Sigma_1 <- matrix(c(sig_11 ^ 2, rho_1 * sig_11 * sig_12, rho_1 * sig_11 * sig_12, sig_12 ^ 2), nrow = 2
det_Sigma_1 <- sig_11 ^ 2 * sig_12 ^ 2 * (1 - rho_1 ^ 2)
det_Sigma_1 <- sig_11 ^ 2 * sig_12 ^ 2 * (1 - rho_1 ^ 2)
Sigma_1 <- matrix(c(sig_11 ^ 2, rho_1 * sig_11 * sig_12, rho_1 * sig_11 * sig_12, sig_12 ^ 2), nrow = 2)
inv_Sigma_1 <- 1 / det_Sigma_1 * matrix(c(sig_12 ^ 2, - rho_1 * sig_11 * sig_12, - rho_1 * sig_11 * sig_12, sig_11 ^ 2), nrow = 2)
mu_0 <- c(mu_01, mu_02)
# Class 0a helpers
mu_0a <- c(mu_01a, mu_02a)
Sigma_0a <- matrix(c(sig_01a ^ 2, rho_0 * sig_01a * sig_02a, rho_0 * sig_01a * sig_02a, sig_02a ^ 2), nrow = 2)
# Class 0a helpers
mu_0a <- c(mu_01a, mu_02a)
Sigma_0 <- matrix(c(sig_01 ^ 2, rho_0 * sig_01 * sig_02, rho_0 * sig_01 * sig_02, sig_02 ^ 2), nrow = 2)
det_Sigma_0  <- sig_01 ^ 2 * sig_02 ^ 2 * (1 - rho_0 ^ 2)
inv_Sigma_0 <- 1 / det_Sigma_0 * matrix(c(sig_02 ^ 2, - rho_0 * sig_01 * sig_02, - rho_0 * sig_01 * sig_02, sig_01 ^ 2), nrow = 2)
# Class 0b helpers
mu_0b <- c(mu_01b, mu_02b)
#library()
# Data Simulation:
# Class 1 parameters
pi_1 <- 0.5
mu_11 <- 4
mu_12 <- 0
sig_11 <- sqrt(2)
sig_12 <- 2
rho_1 <- 0
# Class 0 parameters , choosen randomly with prob 1/2 of 2 buckets
pi_0 <- 1 - pi_1
pi_0a <- 0.5
pi_0b <- 0.5
mu_01a <- 1
mu_02a <- 4
mu_01b <- 1
mu_02b <- -4
sig_01 <- sqrt(2)
sig_02 <- sqrt(2)
rho_0 <- 0
# Class 1 helpers
mu_1 <- c(mu_11, mu_12)
Sigma_1 <- matrix(c(sig_11 ^ 2, rho_1 * sig_11 * sig_12, rho_1 * sig_11 * sig_12, sig_12 ^ 2), nrow = 2)
det_Sigma_1 <- sig_11 ^ 2 * sig_12 ^ 2 * (1 - rho_1 ^ 2)
inv_Sigma_1 <- 1 / det_Sigma_1 * matrix(c(sig_12 ^ 2, - rho_1 * sig_11 * sig_12, - rho_1 * sig_11 * sig_12, sig_11 ^ 2), nrow = 2)
# Class 0a helpers
mu_0a <- c(mu_01a, mu_02a)
Sigma_0 <- matrix(c(sig_01 ^ 2, rho_0 * sig_01 * sig_02, rho_0 * sig_01 * sig_02, sig_02 ^ 2), nrow = 2)
det_Sigma_0  <- sig_01 ^ 2 * sig_02 ^ 2 * (1 - rho_0 ^ 2)
inv_Sigma_0 <- 1 / det_Sigma_0 * matrix(c(sig_02 ^ 2, - rho_0 * sig_01 * sig_02, - rho_0 * sig_01 * sig_02, sig_01 ^ 2), nrow = 2)
# Class 0b helpers
mu_0b <- c(mu_01b, mu_02b)
#
# # Conic section
# A <- sig_12^2/abs(det_Sigma_1)-sig_12^2/abs(det_Sigma_0)
# B <- -2 *(rho_1 * sig_11 * sig_12 / abs(det_Sigma_1) - rho_0 * sig_01 * sig_02 / abs(det_Sigma_0) )
# C <- sig_11^2/abs(det_Sigma_1)-sig_01^2/abs(det_Sigma_0)
#
# det_conic <- B ^ 2 - 4 * A * C
# conic <- "parabola"
# if(det_conic>0){
#   conic <- "hyperbola"
# } else if(det_conic<0){
#   conic <- "ellipse"
# }
# (paste("decision boundary is an",conic))
#
# discriminant_boundary <- function(x1, x2){
#   # Boundary decision equation (equalizing the two discriminant equations)
#   delta_1 <- log(pi_1) -1/2 * log(abs(det_Sigma_1)) -1/2 * sum(t(c(x1 - mu_11, x2 - mu_12)) %*% inv_Sigma_1 %*% c(x1 - mu_11, x2 - mu_12))
#   delta_0 <- log(pi_0) -1/2 * log(abs(det_Sigma_0)) -1/2 * sum(t(c(x1 - mu_01, x2 - mu_02)) %*% inv_Sigma_0 %*% c(x1 - mu_01, x2 - mu_02))
#
#   delta_1 - delta_0
# }
#
# discriminant_boundary_V <- Vectorize(discriminant_boundary)
x <- seq(-6, 6, len = 600)
y <- seq(-6, 6, len = 600)
#z <- outer(x, y, discriminant_boundary_V)
# Sampling n data from the given distrib
set.seed(6)
n <- 10000
n1 <- rbinom(1, n, pi_1)
n0 <- n - n1
# getting dist of 0a and 0b
n0a <- rbinom(1, n0, pi_0a)
n0b <- n - n0a
# Class 1
class_1 <- MASS::mvrnorm(n1, mu_1, Sigma_1)
class_1 <- tibble(Y=1, x1 = class_1[,1], x2 = class_1[,2])
# Class 0a
class_0a <- MASS::mvrnorm(n0a, mu_0a, Sigma_0)
class_0a <- tibble(Y=0, x1 = class_0[,1], x2 = class_0[,2])
#library()
# Data Simulation:
# Class 1 parameters
pi_1 <- 0.5
mu_11 <- 4
mu_12 <- 0
sig_11 <- sqrt(2)
sig_12 <- 2
rho_1 <- 0
# Class 0 parameters , choosen randomly with prob 1/2 of 2 buckets
pi_0 <- 1 - pi_1
pi_0a <- 0.5
pi_0b <- 0.5
mu_01a <- 1
mu_02a <- 4
mu_01b <- 1
mu_02b <- -4
sig_01 <- sqrt(2)
sig_02 <- sqrt(2)
rho_0 <- 0
# Class 1 helpers
mu_1 <- c(mu_11, mu_12)
Sigma_1 <- matrix(c(sig_11 ^ 2, rho_1 * sig_11 * sig_12, rho_1 * sig_11 * sig_12, sig_12 ^ 2), nrow = 2)
det_Sigma_1 <- sig_11 ^ 2 * sig_12 ^ 2 * (1 - rho_1 ^ 2)
inv_Sigma_1 <- 1 / det_Sigma_1 * matrix(c(sig_12 ^ 2, - rho_1 * sig_11 * sig_12, - rho_1 * sig_11 * sig_12, sig_11 ^ 2), nrow = 2)
# Class 0a helpers
mu_0a <- c(mu_01a, mu_02a)
Sigma_0 <- matrix(c(sig_01 ^ 2, rho_0 * sig_01 * sig_02, rho_0 * sig_01 * sig_02, sig_02 ^ 2), nrow = 2)
det_Sigma_0  <- sig_01 ^ 2 * sig_02 ^ 2 * (1 - rho_0 ^ 2)
inv_Sigma_0 <- 1 / det_Sigma_0 * matrix(c(sig_02 ^ 2, - rho_0 * sig_01 * sig_02, - rho_0 * sig_01 * sig_02, sig_01 ^ 2), nrow = 2)
# Class 0b helpers
mu_0b <- c(mu_01b, mu_02b)
#
# # Conic section
# A <- sig_12^2/abs(det_Sigma_1)-sig_12^2/abs(det_Sigma_0)
# B <- -2 *(rho_1 * sig_11 * sig_12 / abs(det_Sigma_1) - rho_0 * sig_01 * sig_02 / abs(det_Sigma_0) )
# C <- sig_11^2/abs(det_Sigma_1)-sig_01^2/abs(det_Sigma_0)
#
# det_conic <- B ^ 2 - 4 * A * C
# conic <- "parabola"
# if(det_conic>0){
#   conic <- "hyperbola"
# } else if(det_conic<0){
#   conic <- "ellipse"
# }
# (paste("decision boundary is an",conic))
#
# discriminant_boundary <- function(x1, x2){
#   # Boundary decision equation (equalizing the two discriminant equations)
#   delta_1 <- log(pi_1) -1/2 * log(abs(det_Sigma_1)) -1/2 * sum(t(c(x1 - mu_11, x2 - mu_12)) %*% inv_Sigma_1 %*% c(x1 - mu_11, x2 - mu_12))
#   delta_0 <- log(pi_0) -1/2 * log(abs(det_Sigma_0)) -1/2 * sum(t(c(x1 - mu_01, x2 - mu_02)) %*% inv_Sigma_0 %*% c(x1 - mu_01, x2 - mu_02))
#
#   delta_1 - delta_0
# }
#
# discriminant_boundary_V <- Vectorize(discriminant_boundary)
x <- seq(-6, 6, len = 600)
y <- seq(-6, 6, len = 600)
#z <- outer(x, y, discriminant_boundary_V)
# Sampling n data from the given distrib
set.seed(6)
n <- 10000
n1 <- rbinom(1, n, pi_1)
n0 <- n - n1
# getting dist of 0a and 0b
n0a <- rbinom(1, n0, pi_0a)
n0b <- n - n0a
# Class 1
class_1 <- MASS::mvrnorm(n1, mu_1, Sigma_1)
class_1 <- tibble(Y=1, x1 = class_1[,1], x2 = class_1[,2])
# Class 0a
class_0a <- MASS::mvrnorm(n0a, mu_0a, Sigma_0)
class_0a <- tibble(Y=0, x1 = class_0a[,1], x2 = class_0a[,2])
# Class 0b
class_0b <- MASS::mvrnorm(n0b, mu_0b, Sigma_0)
class_0b <- tibble(Y=0, x1 = class_0b[,1], x2 = class_0b[,2])
class_dat <- bind_rows(class_1, class_0a, class_0b) %>%
mutate(Y = as.factor(Y))
class_dat
n
n1
n0
n0a
n0b
#library()
# Data Simulation:
# Class 1 parameters
pi_1 <- 0.5
mu_11 <- 4
mu_12 <- 0
sig_11 <- sqrt(2)
sig_12 <- 2
rho_1 <- 0
# Class 0 parameters , choosen randomly with prob 1/2 of 2 buckets
pi_0 <- 1 - pi_1
pi_0a <- 0.5
pi_0b <- 0.5
mu_01a <- 1
mu_02a <- 4
mu_01b <- 1
mu_02b <- -4
sig_01 <- sqrt(2)
sig_02 <- sqrt(2)
rho_0 <- 0
# Class 1 helpers
mu_1 <- c(mu_11, mu_12)
Sigma_1 <- matrix(c(sig_11 ^ 2, rho_1 * sig_11 * sig_12, rho_1 * sig_11 * sig_12, sig_12 ^ 2), nrow = 2)
det_Sigma_1 <- sig_11 ^ 2 * sig_12 ^ 2 * (1 - rho_1 ^ 2)
inv_Sigma_1 <- 1 / det_Sigma_1 * matrix(c(sig_12 ^ 2, - rho_1 * sig_11 * sig_12, - rho_1 * sig_11 * sig_12, sig_11 ^ 2), nrow = 2)
# Class 0a helpers
mu_0a <- c(mu_01a, mu_02a)
Sigma_0 <- matrix(c(sig_01 ^ 2, rho_0 * sig_01 * sig_02, rho_0 * sig_01 * sig_02, sig_02 ^ 2), nrow = 2)
det_Sigma_0  <- sig_01 ^ 2 * sig_02 ^ 2 * (1 - rho_0 ^ 2)
inv_Sigma_0 <- 1 / det_Sigma_0 * matrix(c(sig_02 ^ 2, - rho_0 * sig_01 * sig_02, - rho_0 * sig_01 * sig_02, sig_01 ^ 2), nrow = 2)
# Class 0b helpers
mu_0b <- c(mu_01b, mu_02b)
#
# # Conic section
# A <- sig_12^2/abs(det_Sigma_1)-sig_12^2/abs(det_Sigma_0)
# B <- -2 *(rho_1 * sig_11 * sig_12 / abs(det_Sigma_1) - rho_0 * sig_01 * sig_02 / abs(det_Sigma_0) )
# C <- sig_11^2/abs(det_Sigma_1)-sig_01^2/abs(det_Sigma_0)
#
# det_conic <- B ^ 2 - 4 * A * C
# conic <- "parabola"
# if(det_conic>0){
#   conic <- "hyperbola"
# } else if(det_conic<0){
#   conic <- "ellipse"
# }
# (paste("decision boundary is an",conic))
#
# discriminant_boundary <- function(x1, x2){
#   # Boundary decision equation (equalizing the two discriminant equations)
#   delta_1 <- log(pi_1) -1/2 * log(abs(det_Sigma_1)) -1/2 * sum(t(c(x1 - mu_11, x2 - mu_12)) %*% inv_Sigma_1 %*% c(x1 - mu_11, x2 - mu_12))
#   delta_0 <- log(pi_0) -1/2 * log(abs(det_Sigma_0)) -1/2 * sum(t(c(x1 - mu_01, x2 - mu_02)) %*% inv_Sigma_0 %*% c(x1 - mu_01, x2 - mu_02))
#
#   delta_1 - delta_0
# }
#
# discriminant_boundary_V <- Vectorize(discriminant_boundary)
x <- seq(-6, 6, len = 600)
y <- seq(-6, 6, len = 600)
#z <- outer(x, y, discriminant_boundary_V)
# Sampling n data from the given distrib
set.seed(6)
n <- 10000
n1 <- rbinom(1, n, pi_1)
n0 <- n - n1
# getting dist of 0a and 0b
n0a <- rbinom(1, n0, pi_0a)
n0b <- n0 - n0a
# Class 1
class_1 <- MASS::mvrnorm(n1, mu_1, Sigma_1)
class_1 <- tibble(Y=1, x1 = class_1[,1], x2 = class_1[,2])
# Class 0a
class_0a <- MASS::mvrnorm(n0a, mu_0a, Sigma_0)
class_0a <- tibble(Y=0, x1 = class_0a[,1], x2 = class_0a[,2])
# Class 0b
class_0b <- MASS::mvrnorm(n0b, mu_0b, Sigma_0)
class_0b <- tibble(Y=0, x1 = class_0b[,1], x2 = class_0b[,2])
class_dat <- bind_rows(class_1, class_0a, class_0b) %>%
mutate(Y = as.factor(Y))
class_dat
# Predict with QDA on a grid
class_dat <- bind_rows(class_1, class_0a, class_0b) %>%
mutate(Y = as.factor(Y))
qda_class <- MASS::qda(Y~x1+x2, data=class_dat)
lda_class <- MASS::lda(Y~x1+x2, data=class_dat)
lda_2nd_class <- MASS::lda(Y~x1+x2+I(x1*x2)+I(x1^2)+I(x2^2), data=class_dat)
logreg_class <- glm(Y~x1+x2+I(x1*x2)+I(x1^2)+I(x2^2), data=class_dat, family="binomial")
density_qda <- expand.grid(x1 = x, x2 = y) %>% as_tibble()
qda_pred <- predict(qda_class, density_qda)
lda_pred <- predict(lda_class, density_qda)
lda_2nd_pred <- predict(lda_2nd_class, density_qda)
logreg_pred <- broom::augment(logreg_class, newdata = density_qda, type.predict = "response")
qda_fit <- tibble(class_qda = as.numeric(as.character(qda_pred$class)),
class_lda = as.numeric(as.character(lda_pred$class)),
class_lda_2nd = as.numeric(as.character(lda_2nd_pred$class)),
class_logreg_2nd = logreg_pred$.fitted) %>%
mutate(class_logreg_2nd = if_else(class_logreg_2nd>0.5, 1, 0))
density_qda <- bind_cols(density_qda, qda_fit)
oracle_qda <- bind_cols(density_qda, tibble(oracle = t(matrix(z, nrow=1))))
# Predict with QDA on a grid
class_dat <- bind_rows(class_1, class_0a, class_0b) %>%
mutate(Y = as.factor(Y))
qda_class <- MASS::qda(Y~x1+x2, data=class_dat)
lda_class <- MASS::lda(Y~x1+x2, data=class_dat)
lda_2nd_class <- MASS::lda(Y~x1+x2+I(x1*x2)+I(x1^2)+I(x2^2), data=class_dat)
logreg_class <- glm(Y~x1+x2+I(x1*x2)+I(x1^2)+I(x2^2), data=class_dat, family="binomial")
density_qda <- expand.grid(x1 = x, x2 = y) %>% as_tibble()
qda_pred <- predict(qda_class, density_qda)
lda_pred <- predict(lda_class, density_qda)
lda_2nd_pred <- predict(lda_2nd_class, density_qda)
logreg_pred <- broom::augment(logreg_class, newdata = density_qda, type.predict = "response")
qda_fit <- tibble(class_qda = as.numeric(as.character(qda_pred$class)),
class_lda = as.numeric(as.character(lda_pred$class)),
class_lda_2nd = as.numeric(as.character(lda_2nd_pred$class)),
class_logreg_2nd = logreg_pred$.fitted) %>%
mutate(class_logreg_2nd = if_else(class_logreg_2nd>0.5, 1, 0))
density_qda <- bind_cols(density_qda, qda_fit)
# oracle_qda <- bind_cols(density_qda, tibble(oracle = t(matrix(z, nrow=1))))
ggplot(class_dat) +
geom_point(aes(x=x1, y=x2, colour = Y), alpha = 0.35) +
scale_colour_manual(values = c("dodgerblue", "orange")) +
geom_contour(data = density_qda, aes(x=x1, y=x2, z = class_qda), col = 'darkgrey') +
geom_contour(data = density_qda, aes(x=x1, y=x2, z = class_lda), col = 'darkred') +
geom_contour(data = density_qda, aes(x=x1, y=x2, z = class_lda_2nd), col = 'darkgreen') +
geom_contour(data = density_qda, aes(x=x1, y=x2, z = class_logreg_2nd), col = 'darkblue')
#+ geom_contour(data = oracle_qda, aes(x=x1, y=x2, z = oracle), breaks = c(0), col = 'purple')
