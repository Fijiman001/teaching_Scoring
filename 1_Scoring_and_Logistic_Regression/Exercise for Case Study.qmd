---
title: "Scoring 1: Case Study revision"
author: "Alexander Koehler"
bibliography: references.bib
link-citations: true
format:
  html:
    theme: 
       light: cerulean
    # theme: darkly
    # highlight: espresso
    code-copy: true
    code-fold: true
    df-print: paged
    include-in-header: mathjax.html
    number-sections: true
    toc: true
    toc_depth: 3
    toc_float: yes
    toc-location: left
    fontsize: 10pt
    mainfont: "Helvetica Neue"
execute: 
  #cache: true
  warning: false
editor: visual
fontsize: 11pt
---

```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(tidyverse)
```

## Exercise 1: simulating Data and doing Data Analysis

## Other simulated data sets

Simulate other toy data sets, and plot decision boundaries for classifiers of your choice. For example:

-   Data set 1:
    -   Class 0: mixture (ie two buckets a,b chosen randomly with probability $\frac{1}{2}$) of Gaussian $\mu_{0a}=\begin{bmatrix} 1  \\ 4  \end{bmatrix}$ or $\mu_{0b}=\begin{bmatrix} 1  \\ -4  \end{bmatrix}$ and $\Sigma_{0a}=\Sigma_{0b}=\begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix}$

    -   Class 1: Gaussian with $\mu_{1}=\begin{bmatrix} 4  \\ 0  \end{bmatrix}$ and $\Sigma_{1}=\begin{bmatrix} 2 & 0 \\ 0 & 4 \end{bmatrix}$

Linear\|Quadratic Discriminant Analysis are well described in many books, for example in chapter 4, section 4.3 of @hastie2009 or [here](https://rich-d-wilkinson.github.io/MATH3030/8-lda.html#lda).

Assuming we model each class (indexed by $k \in \{0, 1\}$) density as a multivariate Gaussian (in $\mathbb R^{d}$, here $d=2$):

$$f_k(x) = \frac{1}{(2 \pi)^{d/2} |\Sigma_k|^{1/2}} \exp \left( - \frac{1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k) \right)$$ and knowing $\pi_k = \mathbb{P}[Y=k]$ we have:

$$\mathbb{P}[Y=k|X=x]=\frac{f_k(x)\pi_k}{\mathbb{P}[X=x]}$$ Rewriting Bayes Classifier as: $$ f^*(x)=\left\{ \begin{array}{ll}      1 &  \mbox{if } \mathbb{P}[Y=1|X=x]\geq \mathbb{P}[Y=0|X=x]\cr     0 &  \mbox{otherwise}\cr \end{array} \right. $$

The condition to predict $f^*(x)=1$ is:

$$\frac{\mathbb{P}[Y=1|X=x]}{\mathbb{P}[Y=0|X=x]}=\frac{f_1(x)\pi_1}{f_0(x)\pi_0}\geq 1$$

Or taking the log: $$\log(\frac{f_1(x)\pi_1}{f_0(x)\pi_0})\geq 0$$ We have: $$\log(f_k(x)\pi_k)=\log f_k(x) + \log \pi_k =C -\frac{1}{2} \log |\Sigma_k| -\frac{1}{2} (x-\mu_k)^T \Sigma_k^{-1} (x-\mu_k)+\log \pi_k $$ Usually discriminant functions $\delta_k$ are defined as: $$\delta_k(x)=-\frac{1}{2} \log |\Sigma_k| -\frac{1}{2} (x-\mu_k)^T \Sigma_k^{-1} (x-\mu_k)+\log \pi_k $$ So the Bayes Classifier rewrites: $$ f^*(x)=\left\{ \begin{array}{ll}      1 &  \mbox{if } \delta_1(x)\geq \delta_0(x)\cr     0 &  \mbox{otherwise}\cr \end{array} \right. $$ The decision boundary is $\{x|\delta_1(x)=\delta_0(x)\}$. In the most general case ($\Sigma_1 \neq \Sigma_0$) and for $d=2$ this is the equation of a conic section.

Below we experiment with various choices of $\mu_0,\Sigma_0, \mu_1,\Sigma_1$ and $\pi$ for the $d=2$ case.

We plot the theoretical Bayes decision boundary (a conic, either ellipse, hyperbola or parabola) in purple together with the boundary decision of classifiers estimated on simulated data (LDA in red, QDA in grey, LDA/Logistic Regression with quadratic interactions in green/blue):

```{r}
#library()
# Data Simulation:
# Class 1 parameters
pi_1 <- 0.5

mu_11 <- 4 
mu_12 <- 0

sig_11 <- sqrt(2)
sig_12 <- 2
rho_1 <- 0

# Class 0 parameters , choosen randomly with prob 1/2 of 2 buckets
pi_0 <- 1 - pi_1
pi_0a <- 0.5
pi_0b <- 0.5

mu_01a <- 1 
mu_02a <- 4

mu_01b <- 1 
mu_02b <- -4

sig_01 <- sqrt(2)
sig_02 <- sqrt(2)
rho_0 <- 0 

# Class 1 helpers
mu_1 <- c(mu_11, mu_12)
Sigma_1 <- matrix(c(sig_11 ^ 2, rho_1 * sig_11 * sig_12, rho_1 * sig_11 * sig_12, sig_12 ^ 2), nrow = 2)
det_Sigma_1 <- sig_11 ^ 2 * sig_12 ^ 2 * (1 - rho_1 ^ 2)
inv_Sigma_1 <- 1 / det_Sigma_1 * matrix(c(sig_12 ^ 2, - rho_1 * sig_11 * sig_12, - rho_1 * sig_11 * sig_12, sig_11 ^ 2), nrow = 2)

# Class 0a helpers
mu_0a <- c(mu_01a, mu_02a)
Sigma_0 <- matrix(c(sig_01 ^ 2, rho_0 * sig_01 * sig_02, rho_0 * sig_01 * sig_02, sig_02 ^ 2), nrow = 2)
det_Sigma_0  <- sig_01 ^ 2 * sig_02 ^ 2 * (1 - rho_0 ^ 2) 
inv_Sigma_0 <- 1 / det_Sigma_0 * matrix(c(sig_02 ^ 2, - rho_0 * sig_01 * sig_02, - rho_0 * sig_01 * sig_02, sig_01 ^ 2), nrow = 2)

# Class 0b helpers
mu_0b <- c(mu_01b, mu_02b)

# 
# # Conic section
# A <- sig_12^2/abs(det_Sigma_1)-sig_12^2/abs(det_Sigma_0) 
# B <- -2 *(rho_1 * sig_11 * sig_12 / abs(det_Sigma_1) - rho_0 * sig_01 * sig_02 / abs(det_Sigma_0) )
# C <- sig_11^2/abs(det_Sigma_1)-sig_01^2/abs(det_Sigma_0)
# 
# det_conic <- B ^ 2 - 4 * A * C
# conic <- "parabola"
# if(det_conic>0){
#   conic <- "hyperbola"
# } else if(det_conic<0){
#   conic <- "ellipse"
# }
# (paste("decision boundary is an",conic))
# 
# discriminant_boundary <- function(x1, x2){
#   # Boundary decision equation (equalizing the two discriminant equations)
#   delta_1 <- log(pi_1) -1/2 * log(abs(det_Sigma_1)) -1/2 * sum(t(c(x1 - mu_11, x2 - mu_12)) %*% inv_Sigma_1 %*% c(x1 - mu_11, x2 - mu_12))  
#   delta_0 <- log(pi_0) -1/2 * log(abs(det_Sigma_0)) -1/2 * sum(t(c(x1 - mu_01, x2 - mu_02)) %*% inv_Sigma_0 %*% c(x1 - mu_01, x2 - mu_02)) 
#   
#   delta_1 - delta_0
# } 
# 
# discriminant_boundary_V <- Vectorize(discriminant_boundary)

x <- seq(-6, 6, len = 600)
y <- seq(-6, 6, len = 600)
#z <- outer(x, y, discriminant_boundary_V)

# Sampling n data from the given distrib
set.seed(6)
n <- 10000
n1 <- rbinom(1, n, pi_1)
n0 <- n - n1
# getting dist of 0a and 0b
n0a <- rbinom(1, n0, pi_0a)
n0b <- n0 - n0a

# Class 1
class_1 <- MASS::mvrnorm(n1, mu_1, Sigma_1)
class_1 <- tibble(Y=1, x1 = class_1[,1], x2 = class_1[,2])

# Class 0a
class_0a <- MASS::mvrnorm(n0a, mu_0a, Sigma_0)
class_0a <- tibble(Y=0, x1 = class_0a[,1], x2 = class_0a[,2])

# Class 0b
class_0b <- MASS::mvrnorm(n0b, mu_0b, Sigma_0)
class_0b <- tibble(Y=0, x1 = class_0b[,1], x2 = class_0b[,2])
```

```{r, warning=FALSE}

# Predict with QDA on a grid
class_dat <- bind_rows(class_1, class_0a, class_0b) %>%
              mutate(Y = as.factor(Y))
qda_class <- MASS::qda(Y~x1+x2, data=class_dat)
lda_class <- MASS::lda(Y~x1+x2, data=class_dat)
lda_2nd_class <- MASS::lda(Y~x1+x2+I(x1*x2)+I(x1^2)+I(x2^2), data=class_dat)
logreg_class <- glm(Y~x1+x2+I(x1*x2)+I(x1^2)+I(x2^2), data=class_dat, family="binomial")

density_qda <- expand.grid(x1 = x, x2 = y) %>% as_tibble()

qda_pred <- predict(qda_class, density_qda)
lda_pred <- predict(lda_class, density_qda)
lda_2nd_pred <- predict(lda_2nd_class, density_qda)
logreg_pred <- broom::augment(logreg_class, newdata = density_qda, type.predict = "response")

qda_fit <- tibble(class_qda = as.numeric(as.character(qda_pred$class)),
                  class_lda = as.numeric(as.character(lda_pred$class)),
                  class_lda_2nd = as.numeric(as.character(lda_2nd_pred$class)),
                  class_logreg_2nd = logreg_pred$.fitted) %>% 
                  mutate(class_logreg_2nd = if_else(class_logreg_2nd>0.5, 1, 0))

density_qda <- bind_cols(density_qda, qda_fit) 

# oracle_qda <- bind_cols(density_qda, tibble(oracle = t(matrix(z, nrow=1))))

ggplot(class_dat) +
  geom_point(aes(x=x1, y=x2, colour = Y), alpha = 0.35) +
  scale_colour_manual(values = c("dodgerblue", "orange")) +
  geom_contour(data = density_qda, aes(x=x1, y=x2, z = class_qda), col = 'darkgrey') +
  geom_contour(data = density_qda, aes(x=x1, y=x2, z = class_lda), col = 'darkred') +
  geom_contour(data = density_qda, aes(x=x1, y=x2, z = class_lda_2nd), col = 'darkgreen') +
  geom_contour(data = density_qda, aes(x=x1, y=x2, z = class_logreg_2nd), col = 'darkblue')
  #+ geom_contour(data = oracle_qda, aes(x=x1, y=x2, z = oracle), breaks = c(0), col = 'purple') 
```
