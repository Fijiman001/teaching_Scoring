---
title: "Scoring 1: Exam: In-Class part"
author: "Alexander Koehler"
format:
  html:
    theme: 
       light: cerulean
    # theme: darkly
    # highlight: espresso
    code-copy: true
    code-fold: true
    df-print: paged
    number-sections: true
    toc: true
    toc_depth: 3
    toc_float: yes
    toc-location: left
    fontsize: 10pt
    mainfont: "Helvetica Neue"
execute: 
  #cache: true
  warning: false
editor: visual
fontsize: 11pt
---

```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(tidyverse)
```

# Scoring In-Class Part

## Exercise 3:

Loading data

```{r}
data_fin_exam <- readRDS('C:/Users/Alex/Documents/GitHub/Scoring_Alex/teaching_Scoring/exam/data/data_fin_exam.rds')
 glimpse(data_fin_exam)
```

## Exercise 3: Desbois Ratios

Explore data

Descriptive statistics

show correlated features

showing / plotting individual features "interaction" with the response variable

```{r}
glimpse(data_fin_exam)
```

```{r}
vars_quanti <- names(data_fin_exam %>% select_if(is.numeric))
for(var in vars_quanti){
    var <- as.name(var)
    print(ggplot(data_fin_exam %>%
                     # filter(r17 <0.14) %>% 
                     mutate(Y = as.numeric(Y)-1), aes(x=!! var,y = Y)) +
        geom_jitter(height = 0.1, width = 0) +
        geom_smooth(method = "glm", 
                    formula = y ~ x,
                    method.args = list(family = "binomial"), 
                    se = FALSE,
                    col = "dodgerblue"))
}
```

Our data set nearly only has numbers

Additionally, we see that the financial ratios play a larger role, due to them being ratios and not absolute, thus being adjusted for the companies size. Going forward it makes sense to just look at r1:r37

For some variables, such as r37, we observe outliers to the bottom, in this case meaning that they are very unprofitable compared to their assets: r37: Gross Profit / Total Assets.

We also have an unbalanced dataset, few Y=1 compared to Y=0, making our simple logistic regressions unreliable as shown above

we can also try some binning to see how debt servicing might affect the health of the company

```{r}
# #r21: Interest Expense / EBITDA
# class_width <- 0.1
# (data_fin_exam_binned <- data_fin_exam %>%
#     mutate(balance_bins = cut(r21, breaks = seq(0, 1, class_width),
#                               right = FALSE, dig.lab = 4),
#            min = floor(r21 / class_width) * class_width,
#            max = if_else(r21 == 0 , 1, 
#                          # customers with 0$ balance should be long to [0, width) class
#                          # or be excluded
#                          ceiling(r21 / class_width))  * class_width) %>% group_by(balance_bins, min, max, r21) %>% 
#     summarize(n=n()) %>% 
#     pivot_wider(names_from = r21, values_from = n) %>%
#     replace_na(list(Yes = 0, No = 0)) %>% 
#     mutate(`Mean(r21)` = round(Yes / (Yes + No), 4)))
```

For now, we will use the whole dataset, we might repeat at the end and winsorise the data.

but we will remove collinear features r5, r8, r32. R5 looks very similar to r4, r31 with r28, we can check this with a simple correlation analysis

```{r}
# We deselect the columns not starting with "r".
data_trimmed <- data_fin_exam |> select(Y, starts_with('r')) |> select(-re, -rect)
```

```{r}
cor_matrix <- cor(data_trimmed |> select(-Y), use = "pairwise.complete.obs")
#view(cor_matrix)

# go through the matrix, printing variables that have a correlation with another on of higher than 90%, note: used CHATGPT for this
threshold <- 0.9

# Get the dimensions of the correlation matrix
n <- ncol(cor_matrix)

# Loop through the matrix and print pairs with correlation higher than 90%
for (i in 1:(n-1)) {
  for (j in (i+1):n) {
    if (abs(cor_matrix[i, j]) > threshold) {
      cat("Variables", colnames(cor_matrix)[i], "and", colnames(cor_matrix)[j], 
          "have a correlation of", cor_matrix[i, j], "\n")
    }
  }
}
```

we thus will further trim variables, r5, r6, r8, r32

```{r}
data_trimmed <- data_trimmed |> select(c(-r5,-r6,-r8,-r32))
head(data_trimmed)
```

Below is also the winsorise data for later.

```{r}
# winsorising
data_trimmed_winsorised <- data_trimmed %>%
    mutate(across(r1:r37, ~ DescTools::Winsorize(.x , quantile(.x, probs = c(0.025, 0.975)))))
```

### Fitting a full logistic regression model: r1:r37

```{r}
full_model_desbois <- glm(Y ~ .,
                   data = data_trimmed,
                   family = "binomial") # by default: link = "logit"
summary(full_model_desbois)
```

### Stepwise logistic regression

then use stepwise logistic regression (forward or backward, using the penaliza tion/criterion of your choice) (stepwise_model_desbois) on the data set using variables Y and kept features from exploration step among r1:r37.

```{r}
#define intercept-only model for start
intercept_only <- glm(Y ~ 1, data=data_trimmed, family="binomial")

#define model with all financial ratios
all <- glm(Y ~ ., data=data_trimmed, family="binomial")

# perform forward stepwise regression
forward_aic <- step(intercept_only, direction='forward', test = 'LRT', scope=formula(all), k=2, trace = FALSE)
summary(forward_aic)
```

we see that the optimal formula is:

```{r}
print(forward_aic$formula)
stepwise_model_desbois_formula <- forward_aic$formula
stepwise_model_desbois <- glm(stepwise_model_desbois_formula, data = data_trimmed, family = "binomial")
summary(stepwise_model_desbois)
```

### Comparing full model to stepwise defined model

Compare the full_model_desbois and stepwise_model_desbois using a Likelihood Ratio Test (LRT), ie test if full_model_desbois fits significantly better than the stepwise_model_desbois.

Performing the likelihood ratio test on the two models:

```{r}
#stepwise_model_desbois
#full_model_debois

#using anova from course
anova(stepwise_model_desbois, full_model_desbois, test = "LRT")
```

we see a pr(\>Chi) of 0.8599 such that we cannot reject the null hypothesis of the two models being the same / having the same likelihood. thus the full debois model is not a significant improvement (to the 5% level) to the stepwise and more parsimonius model (the simpler model is statistically "just as good" as the full model)

### Hosmer Lemeshow tests

Compare predicted probabilities to observed probabilities for stepwise_model_desbois using either Hosmer & Lemeshow test or a Calibration Plot.

```{r}
#Using code from lecture and usingthe stepwise model
library(glmtoolbox)
hltest(stepwise_model_desbois)
```

with a p-value of 0.077027, we would not reject the null hypothesis at the 5% level that the goodness of fit of the model is acceptable. However, this would not be the case at the 10% level, so there is a good amount of doubt that our model is good, perhaps the winsorising that will be attempted later will help.

However, we know that the Hosmer & Lemeshow test is dependent on the choice of Q and the binning performed on the probabilities and can be unreliable. thus we will iterative through the possible Q.

```{r}
#Calibration
check_default_prob <- as_tibble(cbind(fitted=stepwise_model_desbois$fitted.values,
                                      Y = data_trimmed %>% mutate(Y = if_else(Y == "1", 1 , 0)) %>% pull(Y)))
(calibration_data <- check_default_prob %>%
  mutate(bins_prob = cut(fitted, breaks = quantile(fitted,seq(0,1,0.10)), include.lowest = TRUE)) %>%
  group_by(bins_prob) %>%
  summarize(n = n(),
            def = sum(Y),
            no_def = n - def,  
            predict_prob = mean(fitted),
            real_prob = def/n,
            forecast_acc = def / sum(check_default_prob$Y)))
```

plotting for better visualisation:

```{r}
(calib_plot <- ggplot(calibration_data, aes(x = predict_prob, y = real_prob)) +
  geom_point() +
  geom_abline(slope = 1))
```

Looks alright from far away, zooming in:

```{r}
calib_plot + coord_cartesian(xlim=c(0, 0.05), ylim=c(0, 0.05))
```

we see quite a bit of divergence at lower probabilities, maybe our model isn't that good, winsorisation might help.

## Exercise 4: Altman Ratios

-   first, create new predictors closets to altmans

-   ![](images/clipboard-2980047475.png)

```{r}
# r24 = EBITA / total assets

# data_fin_exam was the original dataset
# we create a new variable to be similar to altmans variables.
# here for x2, retained earnings / assets. i.e. re / act called re_act
data_altman <- data_fin_exam |> mutate( re_act = re / act)
# sales / total assets: sale_at = sale / at
data_altman <- data_altman |> mutate( sale_at = sale / at)
#working capital / total assets
data_altman <- data_altman |> mutate( wcap_at = wcap / at)
# marketval of equity / book value of debt
data_altman <- data_altman |> mutate( mktval_altt = mktval / dltt, mktval_altt = ifelse(is.infinite(mktval_altt), NA, mktval_altt))
# removing nas
data_altman <- data_altman |> filter(!is.na(mktval_altt))

head(data_altman)
```

"Secondly, fit a logistic regression model model_altman using only these predictors, then:– give an interpretation for the coefficient X3 = EBIT / Total Assets– assess the “significance” of X3 = EBIT / Total Assets coefficient– give a confidence interval for X3 = EBIT / Total Assets"

```{r}
# model_altman_1 <- glm(Y~z, data = data_trimmed, family = "binomial")
# summary(model_altman)

model_altman <- glm(Y~ wcap_at + re_act + r24 + mktval_altt + sale_at, data = data_altman, family = "binomial")
summary(model_altman)
# summary(model_altman_1)
```

The equivalent $x_3$ parameter in my model would be the variable $r24$, in the simple logistic regression above we see that the $r24$ coefficient is negative and significant to the 5% level (with a p-value of \$ 2.51 \* exp(-11)\$ , so close to $0$ ), so if $r24$ increases, i.e. EBIT / Total Assets increases, our probability of default in the next period goes down. more specifically, by $exp(-3.232)$ `{r} round(exp(as.numeric(-3.232)),2)`.

r24 is significant to the 5% level, as p-value is close to 0.

```{r}
# we can also get the p-value directly from the model / table
summary(model_altman)$coefficients[4,4]
```

confidence interval for r24 is: -4.181898 -2.282970.

```{r}
print( 'for r24:')
confint.default(model_altman)[4,]
```

## Exercise 5: Financial items / Lasso

Creating the Dataframe:

```{r}
undesired_columns <- c(paste0("r", 1:37))
existing_columns <- intersect(undesired_columns, colnames(data_fin_exam))

df_Lasso <- data_fin_exam[, !(colnames(data_fin_exam) %in% existing_columns)]
```

– providing descriptive statistics,– showing correlated features,– showing/plotting individual features “interaction” with the response variable

Below for our variables

```{r}
vars_quanti <- names(df_Lasso %>% select_if(is.numeric))
for(var in vars_quanti){
    var <- as.name(var)
    print(ggplot(df_Lasso %>%
                     # filter(r17 <0.14) %>% 
                     mutate(Y = as.numeric(Y)-1), aes(x=!! var,y = Y)) +
        geom_jitter(height = 0.1, width = 0) +
        geom_smooth(method = "glm", 
                    formula = y ~ x,
                    method.args = list(family = "binomial"), 
                    se = FALSE,
                    col = "dodgerblue"))
}
```

```{r}
cor_matrix <- cor(df_Lasso |> select(-Y), use = "pairwise.complete.obs")
#view(cor_matrix)

# go through the matrix, printing variables that have a correlation with another on of higher than 90%, note: used CHATGPT for this
threshold <- 0.9

# Get the dimensions of the correlation matrix
n <- ncol(cor_matrix)

# Loop through the matrix and print pairs with correlation higher than 90%
for (i in 1:(n-1)) {
  for (j in (i+1):n) {
    if (abs(cor_matrix[i, j]) > threshold) {
      cat("Variables", colnames(cor_matrix)[i], "and", colnames(cor_matrix)[j], 
          "have a correlation of", cor_matrix[i, j], "\n")
    }
  }
}
```

removing correlated variables

Based on the financial point of view and linear combinations, you can remove:

Depreciation (dp) (captured in EBITDA). EBIT (captured in EBITDA). Gross Profit (gp) (redundant when using Net Income). Income Before Extraordinary Items (ib) (captured by Net Income). Long-Term Debt (dltt) (captured by Total Liabilities (lt)). Working Capital (wcap) (can be derived from Current Assets (act) and Current Liabilities (lct)). Receivables (rect) and Inventories (invt) (captured within Current Assets (act) if a more granular view is not needed).

```{r}
df_Lasso <- df_Lasso |> select(-dp, -ebit, -gp, -ib, -dltt, -wcap, -rect, -invt)
```

2nd run:

-   **Capx** (since it is highly correlated with **ppent**).

-   **Lt** (since **at** provides a broader picture).

-   **Lct** (or combine it with **act** into a ratio).

-   **Cogs** (since **sale** gives a more comprehensive view).

-   **Re** (since **seq** gives a broader financial view).

-   **Ni** (since **ebitda** is more operationally focused).

```{r}
df_Lasso <- df_Lasso |> select(-capx, -lt, -lct, -cogs, -re, -ni)
```

box-plots and winsorization:

```{r, echo = TRUE, include = FALSE, message = FALSE}
library("DescTools")

# we need to make the df numeric for box plots to work
df_Lasso_numeric <- df_Lasso %>%
  mutate(across(everything(), as.numeric))

# Reshape the data from wide to long format
df_long <- df_Lasso_numeric %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value")

# Plot the boxplots
ggplot(df_long, aes(x = Variable, y = Value)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + # Rotate x-axis labels for readability
  labs(x = "Variables", y = "Values", title = "Boxplots of Variables in df_Lasso")
```

We decide to winsorise the data for the lasso as we observe quite a few outliers and so should have better data.

```{r}
# ensure data is numeric

# # repeating the code for box-plots but with the winsorzed data
# df_Lasso_winsorised <- df_Lasso %>% 
#   select(-Y) %>%
#   mutate(across(everything(), as.numeric)) %>%
#   mutate(across(everything(), ~ DescTools::Winsorize(.x, quantile(probs = c(0.025, 0.975)))))
# 
# # Add back the 'Y' column if needed
# df_Lasso_winsorised <- bind_cols(df_Lasso_winsorised, Y = df_Lasso$Y)
# 
# # View the winsorized data
# head(df_Lasso_winsorised)

# I can~t seem to make the code work at this moment, correct
```

```{r}
# df_Lasso_numeric <- df_Lasso_winsorised %>%
#   mutate(across(everything(), as.numeric))
# 
# # Reshape the data from wide to long format
# df_long <- df_Lasso_numeric %>%
#   pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value")
# 
# # Plot the boxplots
# ggplot(df_long, aes(x = Variable, y = Value)) +
#   geom_boxplot() +
#   theme(axis.text.x = element_text(angle = 90, hjust = 1)) + # Rotate x-axis labels for readability
#   labs(x = "Variables", y = "Values", title = "Boxplots of Variables in df_Lasso")
```

```{r}
#full_model_items
full_model_items <- glm(Y~., data = df_Lasso, family=binomial())
summary(full_model_items)
```

Thenusingfunction glmnet::cv.glmnet or more conveniently glmnetUtils::cv.glmnet as shown in lesson 2 (for example in the cross-validation function defined at the end of the lesson): select a “best value” for the lasso parameter “lambda” giving a penalized model lasso_model_items.

Then using the function glmnet::cv.glmnet or more conveniently glmnetUtils::cv.glmnet as shown in lesson 2 (for example in the cross-validation function defined at the end of the lesson): select a “best value” for the lasso parameter “lambda” giving a penalized model lasso_model_items.

This is due the the lasso specification adding the penalisation term to the MSE formula, thus we increase the MSE when increasing the number of varaibles we use in out model, when $\lambda \neq 0$. Thus, as we are minimising $MSE$ we will shrink the parameters as our penalisation parameter $\lambda$ grows, reducing overfitting.

```{r}
Y <- df_Lasso %>% pull(Y)
lasso_model_items <- glmnetUtils::glmnet(Y ~ ., data=df_Lasso, family="binomial", alpha=1)

plot(lasso_model_items)
```

we have to choose a lambda parameter to then create our model. For this we use the automatic selection provided by glmnet, it will calculate the MSE of all models and look at the optimal MSE and $\lambda$ combination. Giving us the following model / coefficient and variable combination below.

```{r}
lasso_model_items_cv <- glmnetUtils::cv.glmnet(Y ~ ., data=df_Lasso, family="binomial", alpha=1, type.measure = "auc")

# Predicted probs will be run below in the ROC and AUC section

#checking coefficients
lasso_model_items_coef <- coef(lasso_model_items_cv, s = "lambda.1se")

data.frame(name =lasso_model_items_coef@Dimnames[[1]][lasso_model_items_coef@i + 1], coefficient = lasso_model_items_coef@x)
```

## Exercise 6: Model Assessment

we load the test data

```{r}
data_fin_holdout <- readRDS('C:/Users/Alex/Documents/GitHub/Scoring_Alex/teaching_Scoring/exam/data/data_fin_holdout.rds')
 glimpse(data_fin_holdout)
 
#creating the altman predictors:
data_fin_holdout <- data_fin_holdout |> mutate( re_act = re / act)
# sales / total assets: sale_at = sale / at
data_fin_holdout <- data_fin_holdout |> mutate( sale_at = sale / at)
#working capital / total assets
data_fin_holdout <- data_fin_holdout |> mutate( wcap_at = wcap / at)
# marketval of equity / book value of debt
data_fin_holdout <- data_fin_holdout |> mutate( mktval_altt = mktval / dltt, mktval_altt = ifelse(is.infinite(mktval_altt), NA, mktval_altt))
# removing nas
data_fin_holdout <- data_fin_holdout |> filter(!is.na(mktval_altt))

head(data_fin_holdout)
```

Then plot the ROC Curves and compare the AUC of full_model_desbois, step wise_model_desbois, model_altman, full_model_items, lasso_model_items

```{r}
#Plotting the ROC curves

# ROC Curves with ROCR
library("ROCR")    

#we can take Y values from any dataset as they are the same over the datasets

# full_model_desbois
full_model_desbois_predict <- predict(full_model_desbois, newdata=data_fin_holdout, type="response")
pred <- prediction(full_model_desbois_predict, data_fin_holdout$Y)
# pred <- prediction(full_model_desbois, data_trimmed$Y)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf, main="ROC curve Admissions", xlab="Specificity",
     ylab="Sensitivity", col = "darkorange")
abline(0, 1) #add a 45 degree line

full_model_desbois_auc <- ROCR::performance(pred, measure = "auc")
full_model_desbois_auc <- full_model_desbois_auc@y.values[[1]]

# stepwise_model_desbois,
stepwise_model_desbois_predict <- predict(stepwise_model_desbois, newdata=data_fin_holdout, type="response")
pred <- prediction(stepwise_model_desbois_predict, data_fin_holdout$Y)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf, add = TRUE, main="ROC curve Admissions", xlab="Specificity", ylab="Sensitivity", col = "darkolivegreen")

# stepwise_model_desbois_predict <- ROCR::prediction(stepwise_model_desbois$fitted.values, data_fin_holdout$Y)
stepwise_model_desbois_auc <- ROCR::performance(pred, measure = "auc")
stepwise_model_desbois_auc <- stepwise_model_desbois_auc@y.values[[1]]

# model_altman
model_altman_predict <- predict(model_altman, newdata=data_fin_holdout, type="response")
pred <- prediction(model_altman_predict, data_fin_holdout$Y)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf, add = TRUE, main="ROC curve Admissions", xlab="Specificity", ylab="Sensitivity", col = "plum4")

# model_altman_predict <- ROCR::prediction(model_altman$fitted.values, data_fin_holdout$Y)
model_altman_auc <- ROCR::performance(pred, measure = "auc")
model_altman_auc <- model_altman_auc@y.values[[1]]

# full_model_items
full_model_items_predict <- predict(full_model_items, newdata=data_fin_holdout, type="response")
pred <- prediction(full_model_items_predict, data_fin_holdout$Y)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf, add = TRUE, main="ROC curve Admissions", xlab="Specificity", ylab="Sensitivity", col = "darkblue")

full_model_items_auc <- ROCR::performance(pred, measure = "auc")
full_model_items_auc <- full_model_items_auc@y.values[[1]]


# lasso_model_items
full_model_items_predict <- as.vector(predict(lasso_model_items_cv, newdata = data_fin_holdout, s = lasso_model_items_cv$lambda.1se, type = "response"))
pred <- prediction(full_model_items_predict, data_fin_holdout$Y)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf, add = TRUE, main="ROC curve Admissions", xlab="Specificity", ylab="Sensitivity", col = "dodgerblue")

# lasso_model_items_predict <- ROCR::prediction(lasso_model_items$fitted.values, df_Lasso$Y)
lasso_model_items_auc <- ROCR::performance(pred, measure = "auc")
lasso_model_items_auc <- lasso_model_items_auc@y.values[[1]]

#plot
legend(0.6,0.6,
       c('full_model_desbois', 'stepwise_model_desbois', 'model_altman', 'full_model_items', 'lasso_model_items'),
       col=c("darkorange", "darkolivegreen", "plum4", "darkblue", "dodgerblue"),lwd=3)

#AUC measures
auc_table <- table(full_model_desbois_auc, stepwise_model_desbois_auc, model_altman_auc, full_model_items_auc, lasso_model_items_auc)
head(auc_table)
```

In our case, the lasso model is the best with AUC of 0.932389410187667

## Exercise 7: Decision boundary and data simulation

Simulate a training test of 200 observations (100 for each Class 0/1)

```{r}
#library()
# Data Simulation:
# Class 1 parameters
pi_1 <- 0.5

mu_11 <- 4 
mu_12 <- 0

sig_11 <- sqrt(2)
sig_12 <- 2
rho_1 <- 0

# Class 0 parameters , choosen randomly with prob 1/2 of 2 buckets
pi_0 <- 1 - pi_1
pi_0a <- 0.5
pi_0b <- 0.5

mu_01a <- 1 
mu_02a <- 4

mu_01b <- 1 
mu_02b <- -4

sig_01 <- sqrt(2)
sig_02 <- sqrt(2)
rho_0 <- 0 

# Class 1 helpers
mu_1 <- c(mu_11, mu_12)
Sigma_1 <- matrix(c(sig_11 ^ 2, rho_1 * sig_11 * sig_12, rho_1 * sig_11 * sig_12, sig_12 ^ 2), nrow = 2)
det_Sigma_1 <- sig_11 ^ 2 * sig_12 ^ 2 * (1 - rho_1 ^ 2)
inv_Sigma_1 <- 1 / det_Sigma_1 * matrix(c(sig_12 ^ 2, - rho_1 * sig_11 * sig_12, - rho_1 * sig_11 * sig_12, sig_11 ^ 2), nrow = 2)

# Class 0a helpers
mu_0a <- c(mu_01a, mu_02a)
Sigma_0 <- matrix(c(sig_01 ^ 2, rho_0 * sig_01 * sig_02, rho_0 * sig_01 * sig_02, sig_02 ^ 2), nrow = 2)
det_Sigma_0  <- sig_01 ^ 2 * sig_02 ^ 2 * (1 - rho_0 ^ 2) 
inv_Sigma_0 <- 1 / det_Sigma_0 * matrix(c(sig_02 ^ 2, - rho_0 * sig_01 * sig_02, - rho_0 * sig_01 * sig_02, sig_01 ^ 2), nrow = 2)

# Class 0b helpers
mu_0b <- c(mu_01b, mu_02b)

x <- seq(-6, 6, len = 600)
y <- seq(-6, 6, len = 600)

# Sampling n data from the given distrib
set.seed(6)
n <- 200
#n1 <- rbinom(1, n, pi_1)
n1 <- 100
n0 <- n - n1 #will be 100 too
# getting dist of 0a and 0b
n0a <- rbinom(1, n0, pi_0a)
n0b <- n0 - n0a

# Class 1
train_class_1 <- MASS::mvrnorm(n1, mu_1, Sigma_1)
train_class_1 <- tibble(Y=1, x1 = train_class_1[,1], x2 = train_class_1[,2])

# Class 0a
train_class_0a <- MASS::mvrnorm(n0a, mu_0a, Sigma_0)
train_class_0a <- tibble(Y=0, x1 = train_class_0a[,1], x2 = train_class_0a[,2])

# Class 0b
train_class_0b <- MASS::mvrnorm(n0b, mu_0b, Sigma_0)
train_class_0b <- tibble(Y=0, x1 = train_class_0b[,1], x2 = train_class_0b[,2])

# Repeating for test
n <- 2000
#n1 <- rbinom(1, n, pi_1)
n1 <- 1000
n0 <- n - n1 #will be 1000 too
# getting dist of 0a and 0b
n0a <- rbinom(1, n0, pi_0a)
n0b <- n0 - n0a

# Class 1
test_class_1 <- MASS::mvrnorm(n1, mu_1, Sigma_1)
test_class_1 <- tibble(Y=1, x1 = test_class_1[,1], x2 = test_class_1[,2])

# Class 0a
test_class_0a <- MASS::mvrnorm(n0a, mu_0a, Sigma_0)
test_class_0a <- tibble(Y=0, x1 = test_class_0a[,1], x2 = test_class_0a[,2])

# Class 0b
test_class_0b <- MASS::mvrnorm(n0b, mu_0b, Sigma_0)
test_class_0b <- tibble(Y=0, x1 = test_class_0b[,1], x2 = test_class_0b[,2])
```

### Fit a logistic regression (Y \~ x1 + x2) on the training set (model1)

```{r}
train_data <- bind_rows(train_class_1, train_class_0a, train_class_0b) %>% mutate(Y = as.factor(Y))
#test dataset
test_data <- bind_rows(test_class_1, test_class_0a, test_class_0b) %>% mutate(Y = as.factor(Y))

# logistic regression
model1 <- glm(Y~x1+x2, data=train_data, family="binomial")

# more complex logistic regression
model2 <- glm(Y~x1+x2+I(x1*x2)+I(x1^2)+I(x2^2), data=train_data, family="binomial")

# Assess the misclassification error on the testing set.
#model 1 prediction
model1_pred <- broom::augment(model1, newdata = test_data, type.predict = "response")
#model 2 prediction
model2_pred <- broom::augment(model2, newdata = test_data, type.predict = "response")

library("ROCR")

# model1
pred <- prediction(model1_pred$.fitted, test_data$Y)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf, main="ROC curve Admissions", xlab="Specificity",
     ylab="Sensitivity", col = "darkorange")
abline(0, 1) #add a 45 degree line

# compute empirical AUC with ROCR for base logit model
auc_model1 <- ROCR::performance(pred, measure = "auc")
auc_model1 <- auc_model1@y.values[[1]]

# model2
pred <- prediction(model2_pred$.fitted, test_data$Y)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf, add = TRUE, main="ROC curve Admissions", xlab="Specificity", ylab="Sensitivity", col = "darkolivegreen")

auc_model2 <- ROCR::performance(pred, measure = "auc")
auc_model2 <- auc_model2@y.values[[1]]

legend(0.6,0.6,
       c('model1', 'model2'),
       col=c("darkorange", "darkolivegreen"),lwd=3)

auc_model1
auc_model2
```

```{r}
model1_error_rate <- broom::augment(model1, data = train_data, newdata = test_data, type.predict = "response", type.residuals = "deviance") %>% mutate(Y = if_else(Y == "BLUE", 0, 1), predict_glm = 1*(.fitted >= 0.5), l01 = if_else(Y==predict_glm, 0, 1))

model2_error_rate <- broom::augment(model2, data = train_data, newdata = test_data, type.predict = "response", type.residuals = "deviance") %>% mutate(Y = if_else(Y == "BLUE", 0, 1), predict_glm = 1*(.fitted >= 0.5), l01 = if_else(Y==predict_glm, 0, 1))

model1_misclassification_error <- model1_error_rate |> summarise(l01 = mean(l01)) |> pull(l01)

model2_misclassification_error <- model2_error_rate |> summarise(l01 = mean(l01)) |> pull(l01)

# Creating Table for results
misclassification_errors <- matrix(c(model1_misclassification_error, model2_misclassification_error), nrow = 2, byrow = TRUE)

# Set row names and column names
rownames(misclassification_errors) <- c("model1", "model2")
colnames(misclassification_errors) <- "Misclassification Error"

# Convert the matrix to a df for better readability
misclassification_errors_df <- as.data.frame(misclassification_errors)

print(misclassification_errors_df)
```

We see model2 performing slightly better, i.e. having a lower misclassification rate. Thus this model is preferred over model1.

### Bayes Classifier and Decision Boundary

#### Derivation of the Bayes Decision Boundary

To derive the Bayes decision boundary for a classification problem, we start by considering the posterior probabilities of two classes, denoted as $Y = 0$ and $Y = 1$. Let the class-conditional densities be $p(x \mid Y = 0)$ and $p(x \mid Y = 1)$, respectively. The Bayes decision rule classifies a new observation $x$ by selecting the class with the higher posterior probability. The decision boundary occurs where these posterior probabilities are equal:

$$ P(Y = 0 \mid X = x) = P(Y = 1 \mid X = x) $$

By applying Bayes' Theorem, this can be rewritten as:

$$ p(x \mid Y = 0) \cdot P(Y = 0) = p(x \mid Y = 1) \cdot P(Y = 1)$$

This equation defines the Bayes decision boundary, which is the set of points where the two classes are equally probable.

#### Multivariate Normal Distribution Case

Assuming that the data generating process for both classes follows a multivariate normal (Gaussian) distribution, we can further simplify this decision rule. In particular, let the covariance matrices for the two classes, $\Sigma_0$ and $\Sigma_1$, be diagonal (i.e., the off-diagonal terms $\sigma_{1,2}$ and $\sigma_{2,1}$ are zero), indicating that the features are uncorrelated.

In this case, the joint probability density function (PDF) for a multivariate normal distribution is given by:

$$ f_{\boldsymbol{X}} (x_1, x_2) = \frac{1}{(2 \pi)^{d/2} (\det(\Sigma))^{1/2}} \exp\left(-\frac{1}{2} (\boldsymbol{x} - \boldsymbol{\mu})^T \Sigma^{-1} (\boldsymbol{x} - \boldsymbol{\mu})\right) $$

where $\boldsymbol{\mu}$ is the mean vector and $\Sigma$ is the covariance matrix. For each class $Y = 0$ and $Y = 1$, the class-conditional distributions follow this form with their respective parameters.

#### Bayesian Classification Rule

Given the assumption of independent features (diagonal covariance matrices), the total probability of an observation belonging to a class can be computed as the product of its marginal probabilities. Thus, the Bayes classifier compares the likelihood of an observation under each class:

$$ \hat{Y} = \begin{cases}  0 & \text{if } p(x \mid Y = 0) \cdot P(Y = 0) > p(x \mid Y = 1) \cdot P(Y = 1) \\ 1 & \text{otherwise} \end{cases} $$

The class with the higher posterior probability will determine the classification outcome.

#### Decision Boundary Plot

The plot below shows the decision boundary that separates the two classes based on their posterior probabilities. This boundary is determined by the equality of the likelihood functions of the two multivariate normal distributions.

```{r}
library(tidyverse)

grid <- expand.grid(x1 = seq(min(test_data$x1), max(test_data$x1), .1), x2 = seq(min(test_data$x2), max(test_data$x2), .1)) |> as_tibble()
# coarse grid for mimicking ESL figures (little dots)
grid_background <- expand.grid(x1 = seq(min(test_data$x1), max(test_data$x1), .2), x2 = seq(min(test_data$x2), max(test_data$x2), .2)) |> as_tibble()


# Define means and covariance matrices
m01 <- c(1, 4)
m02 <- c(1, -4)
m1 <- c(4, 0)
std01 <- matrix(c(2, 0, 0, 2), nrow=2)
std02 <- matrix(c(2, 0, 0, 2), nrow=2)
std1 <- matrix(c(2, 0, 0, 4), nrow=2)

# Function to compute the Gaussian probability density
gaussian_pdf <- function(x, mean, sigma) {
  d <- length(mean)
  coef <- 1 / ((2 * pi)^(d / 2) * sqrt(det(sigma)))
  exponent <- -0.5 * t(x - mean) %*% solve(sigma) %*% (x - mean)
  return(coef * exp(exponent))
}

# function for our bayesian estimator
predict_oracle <- function(x1, x2) {
  x <- c(x1, x2)
  likelihood_class0_1 <- gaussian_pdf(x, m01, std01)
  likelihood_class0_2 <- gaussian_pdf(x, m02, std02)
  likelihood_class1 <- gaussian_pdf(x, m1, std1)

  likelihood_class0 <- 0.5 * (likelihood_class0_1 + likelihood_class0_2)
  
  1 * (likelihood_class0 - likelihood_class1 < 0)
}

predict_oracle_V <- Vectorize(predict_oracle)

grid <- grid %>% mutate(predict_oracle = predict_oracle_V(x1, x2))

grid_background <- grid_background %>% mutate(predict_oracle = predict_oracle_V(x1, x2))

m1_predict_fn <- function(new_data) {
  outcome <- broom::augment(model1, newdata = new_data, type.predict = "response")
  as.vector(outcome$.fitted)}

m2_predict_fn <- function(new_data) {
  outcome <- broom::augment(model2, newdata = new_data, type.predict = "response")
  as.vector(outcome$.fitted)}

grid$m1_predictions <- m1_predict_fn(grid)
grid$m2_predictions <- m2_predict_fn(grid)

ggplot(grid) + 
geom_contour(aes(x = x1, y = x2, z = predict_oracle),
             breaks = 0.5, col = 'darkgrey') +
geom_contour(aes(x = x1, y = x2, z = m1_predictions),
             breaks = 0.5, col = 'darkgreen', linewidth = 1) +
geom_contour(aes(x = x1, y = x2, z = m2_predictions),
             breaks = 0.5, col = 'purple', linewidth = 1) +
geom_point(data = test_data, aes(x = x1, y = x2, col = Y),
           shape = "o", size = 4, stroke = 2, show.legend = FALSE) +
geom_point(data = grid_background,
           aes(x = x1, y = x2, col = as.factor(predict_oracle)),
           shape = 20, size = .05, alpha = .5, show.legend = FALSE) +
scale_colour_manual(values = c("dodgerblue", "orange")) +
theme_void()
```

The decision boundary plot illustrates the classification of two distinct classes, represented by blue and orange circular markers. These points show some overlap, especially in the central region, indicating classification challenges. The **light gray line** represents the **Bayes decision boundary**, which separates the two classes based on posterior probabilities, providing the optimal theoretical boundary where the probabilities of both classes are equal. The **green vertical line** represents **model1** as before, the simple logistic regression model using only two variables, which results in a linear decision boundary. The **purple curve** represents **model2**, the more complex logistic model that includes the original two variables as well as their squared terms and combinations, resulting in a non-linear decision boundary that better adapts to the data distribution. The contrast between the boundaries highlights how including interaction terms and non-linear combinations can lead to a more flexible and accurate classification model compared to a simpler, linear approach. ultimately, we see that the non-linear model2 performs better than model1 and is closer to the Bayes decision boundary.
